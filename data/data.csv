filename,name,otherNameHits,email,phoneNo,experience,original_text,cleaned_text,keywordaccountingTaxation,percentageaccountingTaxation,keywordARAP,percentageARAP,keywordaudit,percentageaudit,keywordconsulting,percentageconsulting,keywordcreditResearch,percentagecreditResearch,keywordinvestmentBanking,percentageinvestmentBanking,keywordMISFPA,percentageMISFPA
abishekrajaram.gmailnaukri.comabishekrajaramgmailDotcom.pdf,Abishek R,[],['[abishekrajaram@gmail.com]'],['8056774758'],2.5 years,"Abishek R Big Data Engineer Address Madurai, TamilNadu, 625002 Phone 805 677 4758 E-mail abishekrajaram@gmail.com Self-motivated, fast and adaptive learner worked in diverse technologies with 2.5 years’ experience. Recognized consistently for performance and contribution to success in deliverables. Currently looking for opportunities where I could enhance my technical skills thereby benefitting the organization. Skills Big Data Technology: HDFS, Hive, Sqoop, Flume Databases: Oracle 11g, Teradata, DB2 Programming Languages: Python (beginner level), SQL JS Frameworks: AngularJS (novice) XML Tools: XML Spy Version Control Tool: Git Other Tools: Putty, WinSCP, MS Office, Text Pad Methodologies: Agile/Scrum Work History 2019-01 - Current Big Data Engineer Infosys Ltd, Chennai, Tamil Nadu Interoperability HealthCare Analytics Domain • TechStack/Env: Hadoop ecosystem, Hive, Sqoop, Spark, Scala, Shell Scripting, Unix. • Ingested raw data in various file format such as XML, JSON, Delimited file into HDFS JIRA (reporting tool) (Hadoop Data File Systems) • Create complex and robust Hive Queries to validate transformation logic involved. • Performed column data mapping between source & target database. • Performed DDL validation, smoke, system and Regression testing. • Experience in Data Analysis, Data Validation, Data Verification and identifying data mismatch. • Involved in creating test scripts and execution of same in qTest . . • Experience in importing and exporting data from relational database into Hadoop cluster using sqoop commands. • Pro-actively involved in multiple STLC phases and worked successfully to complete it within stipulated time frame along with other activities. • Experience writing Hive Queries for analyzing data in Hive warehouse using Hive • Experience in writing and executing Test Plans and Test Cases from Requirements Query Language (HQL). and Design documents. • Well versed in HDFS commands • Experience on UNIX commands and Shell Scripting Education Bachelor of Engineering: Mechanical Engineering PSNA College of Engineering - Dindigul • Graduated with 7.1 GPA 2014-08 - 2018-05 2013-06 - 2014-06 2020-06 High School Diploma TVS Matriculation Hr. Sec. School - Madurai • Graduated with 84% Certifications Infosys Certified BigData Testing Professional 2020-04 Infosys Certified Python Associate 2019-11 Infosys Global Agile Developer Awards and Recognitions • Insta Awards - Mar'2021 - awarded for scaling up quickly in the project. • Insta Awards - Dec'2020 - has been awarded as a find in the project. . . ",abishek big data engineer address madurai tamilnadu phone email selfmotivated fast adaptive learner worked diverse technologies 25 years experience recognized consistently performance contribution success deliverables currently looking opportunities enhance technical skills benefitting organization skills big data technology hdfs hive sqoop flume databases oracle teradata db2 programming languages python beginner level sql js frameworks angularjs novice xml tools xml spy version control tool git tools putty winscp ms office text pad methodologies agilescrum work history current big data engineer infosys ltd chennai tamil nadu healthcare analytics domain techstackenv hadoop ecosystem hive sqoop spark scala shell scripting unix ingested raw data file format xml json delimited file hdfs jira reporting tool hadoop data file systems create complex robust hive queries validate transformation logic involved performed column data mapping source target database performed ddl validation smoke system regression testing experience data analysis data validation data verification identifying data mismatch involved creating test scripts execution qtest experience importing exporting data relational database hadoop cluster sqoop commands proactively involved multiple stlc phases worked successfully complete stipulated time frame activities experience writing hive queries analyzing data hive warehouse hive experience writing executing test plans test cases requirements query language hql design documents versed hdfs commands experience unix commands shell scripting education bachelor engineering mechanical engineering psna college engineering dindigul graduated 71 gpa high school diploma tvs matriculation hr sec school madurai graduated certifications infosys certified bigdata testing professional infosys certified python associate infosys global agile developer awards recognitions insta awards awarded scaling quickly project insta awards awarded find project,[],0,"['database', 'sql', 'python']",2,"['js', 'angular']",2,"['js', 'angular']",2,[],0,[],0,"['hadoop', 'sql', 'python', 'regression', 'analytics', 'database', 'data analysis']",5
aditya23199006gmail.comAditya_Data_Engineer_resume.docx,Aditya Kumar,"['KUMAR', 'AWS Cloud', 'AWS Glue', 'AWS Glue', 'AWS Glue', 'AWS Glue', 'AWS services', 'Serena Dimension', 'Serena Dimension', 'Vijender', 'Kumar Sharma', 'Sharma', 'Anju Sharma', 'Sharma', 'Vyas', 'Aditya Kumar', 'Kumar']",['[aditya23199006@gmail.com]'],['917983823316'],7 years,"ADITYA KUMAR Reach Me At– Email Id: aditya23199006@gmail.com Mobile: +91 7983823316 Analyst Development Sun Life Financial PROFESSIONAL SUMMARY 7 Years of Experience in the Data Warehousing, ETL & analytics projects including AWS Cloud. 3 years of experience in Data Engineering with manipulating, processing & extracting large datasets. Experience in developing data pipelines using AWS Glue, Py Spark, S3, Redshift, Athena etc. Have extensively worked on developing and implementing Data Lake Projects. Domain experience in Finance, Insurance etc. Leading Data Engineering team to develop and deliver solutions on AWS. Ready to relocate anywhere in India or outside India. Creating POC and suggesting solutions. Worked on multiple projects simultaneously and delivered effectively. Understand the business requirement completely based on discussion with Business partners, High Level specifications and implement the data transformation methodologies. Experience providing technical leadership and Mentoring, Tech Assistance to other data engineers. Result oriented, self-starter and able to work with minimum guidance. Highly motivated towards taking independent responsibility as well as ability to take the team forward. Good communication and presentation skills. Technical Skills ETL Tool: AWS Glue, Informatica Pwrctr 10, BDM v10 AWS: Glue, S3, Redshift, Athena, Step Functions, RDS Big Data Tool: Spark Python IDE Tool: Pycharm Languages: SQL Versioning Tool: Git, Bit Bucket Databases & Related tool: Redshift, Oracle 11g, MS-SQL Server, DB2, WS-FTP Professional Experience Company: Sun Life Financial, Gurgaon Project: IFRS 17 - Sunrise Tenure: Feb 2019 - Present Team Member: 25 Description:- Sun Life Financial is a Canada Based Insurance company. Working in a Captive based organization gave me the liberty of understanding the complete aspect of a project from a Technical as well as from a Business front. IFRS (International Financial Reporting Standards) is the reporting standards issued by International Account Standards Board (IASB). IFRS 17 is a global initiative to bring all the financial organization in the world to work on a common reporting standards. Sun Life being a financial organization is adhering to this notice and inculcating these standard into the organization. This project involves creation of Data Lake on AWS using ETL scripts written in Pyspark on AWS Glue. This lake will become source to different LOB’s, which will be used to create reports. Technologies Used: AWS Glue, S3, Postgres, Redshift, PySpark, Bit Bucket, Git, MS-SQL Server, Step Functions, Athena Responsibilities: Developing ETL solutions using multiple AWS services Glue, S3, Athena, Step Functions etc and Spark. Have experience working with Data Modelers and Architecture. Leading team of 5 data Engineer Orchestrated data flow using Step Functions. Build testing framework using Pytest. Project: T550 Filing Process Tenure: July 2018 – Feb 2019 Team Member: 1 Description:- Canada Revenue Agency (CRA) will start accepting various types of form related to Retirement Plans in XML format, abolishing current practice of receiving them by Retirement Savings Plans (RSP) Provider on DVD. Sun Life, being a financial organization, adhering to this circular by executing this project, which will convert these forms to XML format and share it across CRA. Technologies Used: Informatica Powercentre v10, XML Spy Professional, Microsoft SQL Server, Jira Responsibilities: Leading project individually from offshore and implementing project end to end. Working on the crux of requirement, suggesting and developing the ETL solution. Communicating with the onshore partners for the design and development. Creating Change Management request for the code release. Additionally, lead a team of 2 team members on another project related to Data Lake and Ingestion. Project: Data Lake Ingestion and Cognos Replacement Tenure: July 2017 – July 2018 Team Member: 3 Description:- The project was initiated to implement the reporting solution using SSAS/SSIS and Tableau Desktop as Cognos support was going to end. Simultaneously the project also involved the ongoing Data Lake Ingestion processes at Sun Life Financial. The project had Hive as an intermediary stage, which served as Data Lake. Technologies Used: Informatica Powercentre v10, Informatica BDM v10, Oracle 11g, WS-FTP, Microsoft SQL Server, DB2, Hive-Ambari, Jira, Serena Dimension, MVS Zeke Responsibilities: Involved in the initial estimate of resource planning and work allocation. Performed Data Ingestion into Hadoop Data Lake envr. Scheduled Informatica workflows, Ingestion and BDM Jobs using MVS ZEKE. Lead the project multiple times in the absence of Team Lead. Ticket handling and resolving issues during implementation of project on ALM. Worked and handle the complete cycle of the project from requirement gathering, design, prod implementation and warranty support. Deployment of code to higher environment using tools like Jira, Serena Dimension etc. Production implementation of the code and troubleshooting issues while doing so. Simultaneously worked and handled other projects as well apart from working on this one. Company: Cognizant Technology Solutions, Chennai Client: excellus, US Project: EDW Subscriber Corrected Address Tenure: April 2016 – April 2017 Team Member: 5 Client: ing life, japan Tenure: Sept 2014 – March 2016 Team Member: 4 _________________________________________________________________ Personal details: Father’s Name: Mr. Vijender Kumar Sharma Mother’s Name: Late Anju Sharma Date of Birth: 23-June-1990 Educational Qualification B-Tech in Electronics and Communication Vyas Institute of Engineering & Technology, RTU Aggregate: 63.17 % Year of Passing- 2012 H.S.C: 68.6% from Air Force School, Jodhpur (CBSE) S.S.C: 80.2% from Air Force School, Jodhpur (CBSE) Current Employer: Sun Life Financial, Gurgaon Duration: 24 July, 2017 – Present Last Employer: Cognizant Technology Solutions, Chennai Duration: 11 April, 2014 – 25 April, 2017 Dated: 01 July, 2019 Place: Gurgaon (Aditya Kumar) Page 2 of 4",aditya kumar reach email id mobile analyst development sun life financial professional summary years experience data warehousing etl analytics projects including aws cloud years experience data engineering manipulating processing extracting large datasets experience developing data pipelines aws glue py spark s3 redshift athena etc extensively worked developing implementing data lake projects domain experience finance insurance etc leading data engineering team develop deliver solutions aws ready relocate india outside india creating poc suggesting solutions worked multiple projects simultaneously delivered effectively understand business requirement completely based discussion business partners high level specifications implement data transformation methodologies experience providing technical leadership mentoring tech assistance data engineers result oriented selfstarter able work minimum guidance highly motivated taking independent responsibility ability team forward good communication presentation skills technical skills etl tool aws glue informatica pwrctr bdm aws glue s3 redshift athena step functions rds big data tool spark python ide tool pycharm languages sql versioning tool git bit bucket databases related tool redshift oracle mssql server db2 wsftp professional experience company sun life financial gurgaon project ifrs sunrise tenure feb present team member description sun life financial canada based insurance company working captive based organization gave liberty understanding complete aspect project technical business ifrs international financial reporting standards reporting standards issued international account standards board iasb ifrs global initiative bring financial organization world work common reporting standards sun life financial organization adhering notice inculcating standard organization project involves creation data lake aws etl scripts written pyspark aws glue lake source different lobs create reports technologies aws glue s3 postgres redshift pyspark bit bucket git mssql server step functions athena developing etl solutions multiple aws services glue s3 athena step functions etc spark experience working data modelers architecture leading team data engineer orchestrated data flow step functions build testing framework pytest project filing process tenure feb team member description canada revenue agency cra start accepting types form related retirement plans xml format abolishing current practice receiving retirement savings plans rsp provider dvd sun life financial organization adhering circular executing project convert forms xml format share cra technologies informatica powercentre xml spy professional microsoft sql server jira leading project individually offshore implementing project end end working crux requirement suggesting developing etl solution communicating onshore partners design development creating change management request code release additionally lead team team members project related data lake ingestion project data lake ingestion cognos replacement tenure team member description project initiated implement reporting solution ssasssis tableau desktop cognos support going end simultaneously project involved ongoing data lake ingestion processes sun life financial project hive intermediary stage served data lake technologies informatica powercentre informatica bdm oracle wsftp microsoft sql server db2 hiveambari jira serena dimension mvs zeke involved initial estimate resource planning work allocation performed data ingestion hadoop data lake envr scheduled informatica workflows ingestion bdm jobs mvs zeke lead project multiple times absence team lead ticket handling resolving issues implementation project alm worked handle complete cycle project requirement gathering design prod implementation warranty support deployment code higher environment tools like jira serena dimension etc production implementation code issues simultaneously worked handled projects apart working company cognizant technology solutions chennai client excellus project edw subscriber corrected address tenure team member client ing life japan tenure sept team member personal details fathers mr vijender kumar sharma mothers late anju sharma date birth educational qualification btech electronics communication vyas institute engineering technology rtu aggregate year passing hsc air force school jodhpur cbse ssc air force school jodhpur cbse current employer sun life financial gurgaon duration present employer cognizant technology solutions chennai duration dated place gurgaon aditya kumar page,[],0,"['aws', 'database', 'sql', 'python', 'request']",3,['sass'],0,['sass'],0,[],0,[],0,"['hadoop', 'excel', 'pyspark', 'tableau', 'sql', 'python', 'analytics', 'database']",2
ankitmittal437gmail.comAnkit_Mittal_-_Resume.pdf,Ankit Mittal,"['Mittal', 'Ankit Mittal', 'Mittal']",['[ankitmittal437@gmail.com]'],['918218825021'],6+ years,"Ankit Mittal Email: ankitmittal437@gmail.com Phone: +91-8218825021 Profile Summary ● A dynamic professional with 6+ years of career experience in Data Engineering using Azure Cloud and On Premise Infrastructure, Business Intelligence and Data warehousing ● Currently working with MothersonSumi Infotech and Designs on Azure Data Engineering Profile ● Hands on experience in designing and implementing the architecture for Data Engineering Solutions on Azure Cloud and On Premise ● Hands on experience in data warehouse designing and data modelling ● Expertise on working in Azure Services like Azure Data Factory, Azure SQL Database, Azure Data Lake, Azure Synapse Analytics, Azure Functions, Azure Logic Apps, Azure Databricks, Azure Eventhub, Azure Stream Analytics ● Hands on experience in developing Power BI Reports ● Extensive hands-on experience in Performance Tuning of the Data Warehouse solutions ● Hands-on experience of designing and implementing Real-Time Data Processing Solutions using Microsoft Azure services ● Expertise knowledge of working on Manufacturing Domain along with Hospitality, Market Research, City Gas Distribution domains ● Exposure to working on PySpark (Spark + Python) Professional Experience ● MothersonSumi Infotech and Designs ● Ernst and Young (EY) ● Optimus Information Inc. Professional Projects 1. Factory Analytics Solution (Feb 2020 - Present) (Nov 2019 - Feb 2020) (Jul 2015 - Nov 2019) Technology: Azure Data Factory, Azure SQL Database, T-SQL Queries, Azure Synapse Analytics, Azure Databricks, Azure Data Lake, Azure Eventhub, Azure Stream Analytics, PySpark (Spark + Python) Description: It is a product developed for the Manufacturing units to monitor the plants performance using various KPIs. My role in this project is to design the solution on Microsoft Azure Cloud and also integrate the new clients into the existing on-premise solution. 2. Hospitality Data Analytics Services Technology: Azure Data Factory, Microsoft Power BI, Azure SQL Database, SQL Queries, Powershell, Azure Analytics Services, Azure App Service, Azure Power BI Embedded, Azure Function App Description: Designed, developed and deployed a data engineering solution for the hospitality domain client to keep the track of their Sales, Payments, Taxes, etc. This is a fully cloud based solution implemented on Azure. 3. City Gas Distribution Technology: Azure Data Factory, Azure Databricks, Azure Analysis Service, Power BI, PySpark(Python + Spark) Description: This project is designed to do the analysis on the gas distribution for the City Gas Distribution Company. My role in this project is to develop the Power BI Reports and manage the Analysis Services Model. Academic ● B.Tech (CSE) from Lovely Professional University with 8.3 CGPA in 2015. ● Intermediate from CBSE with 79% in 2011. ● High School from CBSE with 88.2% in 2009. Achievements ● Cleared Microsoft certification 70-761 - Querying Data with Transact-SQL ● Cleared Microsoft certification 70-762 - Developing SQL Databases ● Cleared Microsoft certification 70-767 - Implementing a Data Warehouse ● Cleared Microsoft certification DP-200 - Implementing Azure Data Solutions ● Optimus Tee Ten Award, 2016 ● Optimus Tee Ten Award, 2017 ● Optimus Value Award, Self-Led 2018 Personal Profile ● Date of Birth ● Gender ● Languages : 09-Nov-1993 : Male : English, Hindi Declaration I hereby solemnly affirm that all the information furnished above is true to the best of my knowledge and belief. (Ankit Mittal) Date: 10-Aug-2021 ",ankit mittal email phone profile summary dynamic professional years career experience data engineering azure cloud premise infrastructure business intelligence data warehousing currently working mothersonsumi infotech designs azure data engineering profile hands experience designing implementing architecture data engineering solutions azure cloud premise hands experience data warehouse designing data modelling expertise working azure services like azure data factory azure sql database azure data lake azure synapse analytics azure functions azure logic apps azure databricks azure eventhub azure stream analytics hands experience developing power bi reports extensive handson experience performance tuning data warehouse solutions handson experience designing implementing realtime data processing solutions microsoft azure services expertise knowledge working manufacturing domain hospitality market research city gas distribution domains exposure working pyspark spark python professional experience mothersonsumi infotech designs ernst young ey optimus information inc professional projects factory analytics solution feb present nov feb jul nov technology azure data factory azure sql database tsql queries azure synapse analytics azure databricks azure data lake azure eventhub azure stream analytics pyspark spark python description product developed manufacturing units monitor plants performance kpis role project design solution microsoft azure cloud integrate new clients existing onpremise solution hospitality data analytics services technology azure data factory microsoft power bi azure sql database sql queries powershell azure analytics services azure app service azure power bi embedded azure function app description designed developed deployed data engineering solution hospitality domain client track sales payments taxes etc fully cloud based solution implemented azure city gas distribution technology azure data factory azure databricks azure analysis service power bi pysparkpython spark description project designed analysis gas distribution city gas distribution company role project develop power bi reports manage analysis services model academic btech cse lovely professional university 83 cgpa intermediate cbse high school cbse achievements cleared microsoft certification querying data transactsql cleared microsoft certification developing sql databases cleared microsoft certification implementing data warehouse cleared microsoft certification implementing azure data solutions optimus tee award optimus tee award optimus value award selfled personal profile date birth gender languages male english hindi declaration solemnly affirm information furnished true best knowledge belief ankit mittal date,[],0,"['database', 'sql', 'azure', 'python']",14,[],0,[],0,[],0,[],0,"['data processing', 'pyspark', 'sql', 'python', 'analytics', 'database']",7
awinjes.gmailnaukri.comresume.docx,Awin Kumar,"['kumar', 'Mahindra', 'SPS receives', 'peer team', 'Master']","['[-awinsas@gmail.com]', '[awinjes@gmail.com]']","['9665612536', '9013889376', '9972548530']",-,"Awin kumar Summary To strive for Excellence in the field of software development – with dedication, focus, proactive approach, positive attitude and passion and also to utilize my knowledge and skills in the best possible way for the fulfillment of goals. I believe in myself and like doing challenging work, which would satisfy my instinct to learn and do more and having high thinking which is helpful in doing big things and motivates myself. I would anticipate receiving some quality work experience that provides me an insight into the working of an organization like yours, which would enrich my knowledge and fulfil my goals. Technical Skills Languages: SAS,R,python,pyspark Scripting Languages: Shellscript , perlscript, JavaScript, Vbscript, xml, PHP,qlikView RDBMS: ORACLE ,DB2,MYSQL POSTGRESQLSQL SERVER, MS-ACCess Technologies: Mainframe, Asp.net, SAS(STATICAL ANALYSIS OF SYSTEM) Operating System: Windows NT 4.0/2000, Windows 9x, DOS, LINUX,UNIX. WEB APPLICATION: Django,Flask,cherryPy,Pyramid. System A/D Tools: Flow chart and DFD. Database Designing Tools: ER Diagram and Normalization. DBAmongodb-performance tuning,cluster,rplication,sharding,ops-Manager Project Summary Analytics project Total exp: 10.10year 2.4 year in Mainframe(cobol-4year,jcl,db2,cics unix) Analytics project 8.6 year Sas,sasaml6.1,R,spss,qlik view,tableau,Micro strategy python(core python,pandas,numpy,pyspark,REST with Django.Flask and we2py task automation celery with rabbitmq) Perl (core perl,oops) Analytics with shiny, 5 year in spark,pyspark,kafka,rabbitMq,RDD,pig,hive,hbase,sqoop, flume,zookeeper,spark-streaming (Unix, shell script, Perl script,oracle,pl/sql,data warehousing, data model,Autosys)) Project: cisco April 2020 to till now Role: developer (tl) Company name: Tech Mahindra project name :matix cloud 1.static code analysis ,checking venerability,using sonarQube,pylint 2.Python with web application(Django,flask,rest_api) along database (postgresql,mongodb) 3.sas reporting. Project: 3UK- united kingdom March 2019 to Aug 2019 Role: DM Company name: quess corp Environment: AWS,linux (sed,awk,grep,find),SAS 9.3,9.4,enterprise guide, R, python, perl for data analysis,tableau, qlikview,micro strategy Python with Django admin,Rest, CI/CD,rabbit mq,kafka Client: Stanford graduate school of business,USA Domain: ETL Designation: DM Database:DBA-mongodb-performance tuning,cluster,rplication,sharding,ops-Manager SAS: Base sas,advance sas, Macro ,qlikview,sas with REST spss : data representation anlysis Hadoop:PIG latin,Hive,hbase ,mongodb(admin),pyspark MLIB: Bayesian network,regression model time series,neural network.NLP,text mining,image processing Cloud: aws(EC2),azure,google cloud Repository: git,svn.docker,kubernet Description Data is coming from different sourclike hdfs,csv,xml,rdbms file format .i connected with source and manipulating the data using spark sql and then return the data in database. Some data is display in his web portel using python django/flask/web2py or some time using Rest api for storing and retriving the data. Role: Senior technical consultant July 2018 to Dec 2018 Project: Stanford graduate school of business,USA Company name: skrouz technologies pvt.ltd. Environment: AWS,linux (sed,awk,grep,find),SAS 9.3,9.4,enterprise guide :R,python,perl for data analysis,tablue,qlikview,microstrategy, Python with Django admin,Rest, CI/CD,rabbit mq,kafka Client : Stanford graduate school of business,USA Domain: education Designation: Senior technical consultant (Data scientist) Database: DBA=mongodb=performance tuning,cluster,rplication,sharding,ops=Manager Sas: Base sas,advance sas, Macro ,qlikview,sas with REST spss : data representation anlysis Hadoop:PIG latin,Hive,hbase ,mongodb(admin),pyspark MLIB: Bayesian network,regression model time series neural network.NLP,text mining,image processing cloud : aws(EC2),azure,google cloud repository : git,svn.docker,kubernet Data is coming from different sourclike hdfs,csv,xml,rdbms file format .i connected with source and manipulating the data using spark sql and then return the data in database. Some data is display in his web portel using python django/flask/web2py or some time using Rest api for storing and retriving the data. Role: Consultant Oct 2017 - March 2018 Project: ABSA bank ,Johannesburg,South Africa Role: consultant Company name: Cintana technologies Environment : linux (sed,awk,grep,find), Sas: Base sas,advance sas, Macro ,sasaml6.1,sas visual designer(data prepration,scenario rule creation,tunning,de[loyment) R,python,perl for data analysis,tablue,qlikview,micro strategy,rabbitmq,kafka Client: ABSA bank,joburg,south africa Domain: banking & finance Designation: consltant (Data scientist) Database: DBA-mongodb-performance tuning,cluster,rplication,sharding,ops-Manager Sas: Base sas,advance sas, Macro ,qlikview,tableu,micro startgey spss : data representation anlysis Hadoop:PIG latin,Hive,hbase ,mongodb(admin) pyspark neural network cloud: aws(EC2),azure,google cloud repository: git,svn.docker,kubernet Description Data is coming from different sourclike hdfs,csv,xml,rdbms file format .i connected with source and manipulating the data using spark sql and then return the data in database. Some data is display in his web portel using python django/flask/web2py or some time using Rest api for storing and retriving the data. Role: Senior Software Engineer Dec 2016 to 22 Sep 2017 Project: Citi transaction and forcasting engine Role: Senior software engineer. Company name: collebra Environment: linux (sed,awk,grep,find),SAS 9.3,9.4,enterprise guide6.1,R,python for data analysis,tablue,qlikview,micro strategy Client : citi bank Domain : banking & finance Designation: senior Software Engineer (Data scientist) Database:oracle11g,pl/sql Sas : Base sas,advance sas, Macro ,qlikview,tableu,micro spss : data representation anlysis Hadoop:PIG latin,Hive,hbase ,mongodb,pyspark MLIB : Bayesian network,regression model time series,KNN,caratepplier, Cloud: aws(EC2),azure,google cloud repository: git,svn.docker,kubernet Data is coming from different sourclike hdfs,csv,xml,rdbms file format .i connected with source and manipulating the data using spark sql and then return the data in database. Some data is display in his web portel using python django/flask/web2py or some time using Rest api for storing and retriving the data. Project : E2E AML (ANTIMONEY LAUNDARY ) Role: Senior software engineer. Environment: linux (sed,awk,grep,find),SAS 9.3,9.4,enterprise guide6.sas aml 6.11,R,python for data analysis,tablue,qlikview,micro strategy Client: citi bank Domain: banking & finance Designation: senior Software Engineer (Data scientist) Database:oracle11g,pl/sql DBA=mongodb=performance tuning,cluster,rplication,sharding,ops=Manager Sas: Base sas,advance sas, Macro ,sasaml6.1,sas visual designer (data prepration,scenario rule creation,tunning,de[loyment) spss : data representation anlysis Hadoop:PIG latin,Hive,hbase ,mongodb,pyspark MLIB : Bayesian network,regression model time neural netwok,nlp ,text mining cloud : aws(EC2),azure,google cloud repository : git,svn.docker,kubernet Description Data is coming from different sourclike hdfs,csv,xml,rdbms file format .i connected with source and manipulating the data using spark sql and then return the data in database. Some data is display in his web portel using python django/flask/web2py or some time using Rest api for storing and retriving the data. Project : gtpl(global trade profit and loss) ,Airs(Aged inventory reporting system) Fats(Financial Account and trading system),sas compliance Role: Senior software engineer. Environment : linux (sed,awk,grep,find), SAS 9.3,9.4,enterprise guide6.1,sasaml6.1,R,python for data analysis,tablue,qlikview,micro strategy Client: Citi bank Domain: Banking & finance Designation: Senior Software Engineer (Data scientist) Database:oracle11g,pl/sql DBA=mongodb=performance tuning,cluster,rplication,sharding,ops=Manager Sas: Base sas,advance sas, Macro ,qlikview,tableu,rabbitmq spss : data representation anlysis Hadoop:PIG latin,Hive,hbase ,mongodb,pyspark MLIB: Bayesian network,regression model time series,KNN,caratepplier, neural network,nlp .text mining Spark: scala pyspark,Rspark analytics:Rwithmachine learning,Python(ipython,numpy,pandas,metaplotlib language :shellscript,perl,python,R,PL/sql Machine leaarning : regression,multiple regression,times series analysis R with shiny,R with dashboard,R with data mining Description: batch job monitoring and data analysis dashboard creation , cloud: aws(EC2),azure,google cloud repository: git,svn.docker,kubernet Description Data is coming from different sourclike hdfs,csv,xml,rdbms file format .i connected with source and manipulating the data using spark sql and then return the data in database. Some data is display in his web portel using python django/flask/web2py or some time using Rest api for storing and retriving the data. Role: Senior Software Engineer April 2016 to July 2016 Company name: adicco india pvt.ltd Project: Orian2(payment gateway) Role: Senior software engineer. Environment : AIX 7.1 unix(sed,awk,grep,find) ,SAS 9.3,9.4,enterprise guide6.1,R,python for data analysis. Client: Bnp paribas(IBM global business service) Domain: banking & finance Designation: Senior Software Engineer (Data scientist) April 2016 to july 2016 Database: oracle11g,pl/sql DBA=mongodb=performance tuning,cluster,rplication,sharding,ops=Manager sas: Base sas,advance sas, Macro spss : data representation anlysis Haddop: PIG latin,Hive in hadoop,pyspark,rabbitmq Analytics: R withmachine learning,Python(ipython,numpy,pandas,metaplotlib Language: shellscript,perl,python,MFCOBOL,,R,PL/sql Machine leaarning : regression,multiple regression,times series analysis R with shiny,R with dashboard,R with data mining unix using awk,sed and grep Cloud: aws(EC2),azure,google cloud Repository: git,svn.docker,kubernet Description Data is coming from different sourclike hdfs,csv,xml,rdbms file format .i connected with source and manipulating the data using spark sql and then return the data in database. Some data is display in his web portel using python django/flask/web2py or some time using Rest api for storing and retriving the data. Role: Senior Software Engineer Nov 2014 To Jul 2015 Project: General Motor Role: Senior software engineer. Company name: artech info system pvt.ltd. Environment : sun solaris-10 unix(sed,awk,grep,find,Web Scraping) , SAS 9.3,9.4,enterprise guide6.1,R,python for data analysis.qlikview ,tableu Client : General Motor(IBM global business service) Domain : Auto mobile Designation: Senior Software Engineer Nov 2014 to July 2015 Database: Oracle11g,INGRES 9.2 Sas: Base sas,advance sas, Macro haddop :PIG latin,Hive in hadoop,pyspark analytics:Rwithmachine learning,Python(ipython,numpy,pandas,metaplotlib language :shellscript,perl,python,MFCOBOL, c programing,R Cloud : aws(EC2),azure,google cloud Repository : git,svn.docker,kubernet Description The Supply Planning System (SPS), sas(stock allocation system), crr(cclient return request)handles orders and process them, as well as it acts as an intermediary for passing on essential order information to and from other systems within Aftersales. It was developed as part of the After sales System solution for France and Italy. SPS,sas,crr is a client/server distributed system; it runs in a Unix Machine located in Germany and all SPS,sas and crr sites (France, Italy, Turkey, Holland, Hungary) run independently of each other. The SPS,sas,crr screens are written in OpenROAD and the data is stored in an oracle, Ingres Database. The batch programs are written in C, SQL, Perl and COBOL (very few), Sas(base sas,advance sas,Macro) for report generation. The main functionality supported by SPS,sas,crr is Procurement. Its purpose is order handling and processing, as well as acting as an intermediary for passing on essential order information to and from other systems. Parts details come down from DMT/ DDS and the user initializes the SKU. The contracts are then set up in SPS,sas,crr for the appropriate supplier. Once MLE is aware of the part, forecasting begins and suggested orders are generated and sent to SPS. On approval by the user, SPS sends the orders on to the supplier. SPS receives and processes ASN data from the suppliers, before transmitting it on to the Catalyst system to allow for processing of inbound orders. When the parts have been received into the warehouse, Catalyst transmits confirmed receipts data to SPS. Open orders are adjusted accordingly and stocks re-ordered according to requirements and demand Role There are report generation,data analysis & production and support, graphical representation of data using R,SAS,PYTHON,SHINY three applications SPS- supply planning system SAS- Stock allocation system CRR -client return request Report generation for country wise as well as client wise (for 22 country) some production and support kind of activity Batch job activity and implementation of shell script and perl script, c programming,MF cobol compression for different file system solving ticket in production support Data analysis using R ,PYTHON AND SAS,tableu TIME SERIES ANALYSIS: In stock allocation system we allocate and analysis of pass data using trend analysis, Seasonal, Random, Shiny framework with R, Hadoop with R(rmr2,rhdfs,r* hiva,rhbase) Tool used: Rstudio,R with shiny(graphical analytic), hadoop(pig latin,hive) Using R and python for analysis and forecasting. Database backup and recovery: Mysql database recovery & backup postgresql database backup & recovery Oracle database backup & recovery through Rman mongodb database backup and recovery(nosql) Role: Senior Software Engineer Jul 2013 to May 2014 Project: Catarman Company name: artech info system pvt.ltd. Environment: AIX-unix(sed,awk,grep,find) ,SAS 9.3,9.4,enterprise guide6.1,R,python for data analysis. qlikview,tableu Client : catarman(IBM(global business services) Domain : Health care Designation : senior Software Engineer Database :oracle,db2,mongodb,sqoop,casandra.,qlik view DBA-mongodb,performance tuning,cluster,rplication,sharding,ops-Manager Oracle apps (functional) : financial,pl/sql Role There are report generation & data analysis. MTM report generation. Rdur report generation. Data analysis through python.Web Scraping NumPy,Ipython, Metaplotlib, pandas, python and R language for data analysis Sas descriptive stastistical analysis(mean,median mode, sd,) inferential stastistical(correlation,regression,anova test,chi-square testing etc) Tool:unix ,sas,vncviewr,winscp,shellscript,pythonWeb Scrapingenterprise guide 6.1 SAS: 9.3,9.4,base sas ,advance sas ,Macro Administration: SAS management console Support has been added for metadata server clusters Support has added for secures libraries New server management functions have been added Support has been added for grid option set. Support has been add fro grid-launched workspace server. Resource template have added and revised fro server and library definition Operating system: unix,shellscript,perlscript Sas : sas 9.3,9.4base sas,advance sas,Macro Database : oracle 11g, hadoop Project :Catamaran Description Converting the previous sas code like (sas 9.1 and sas 9.2 into sas 9.3,9.4) Creating data set. Connecting to database oracle and extraction of data. Preparing report . Sending to multiple address through crontab ,Job secheduling. As we are getting files from across system, we need to format the files. Bringing up the Data Mapping exercise, which gave a solid base to prepare the Technical Specification Document. In getting all issue clarifications by identifying them and conveying it to Onsite through frequent calls and mails and tracking the open issues to closure. Retrieved tables from databases with PROC SQL using ‘Pass through Facility’. Ad-hoc reports are generated from the analysis dataset as per the requirement of the Statisticians using Proc report, Proc tabulate, Proc print and Data Null. Created analysis datasets from raw datasets using Set, Merge, Sort, Update, Formats, Functions and conditional statements Used Data _Null_ technique for producing highly formatted and highly customized reports. Used SAS Macro facility to produce weekly and monthly reports. Involved in identifying the inconsistency of the data using SAS procedures. Involved in writing code to extract clean and validated Data from tables. Listing & summary reports were generated based on the client requirement. Used SAS ODS for generating reports in specific output formats like RTF, PDF, HTML etc Base sas,advance sas, Macro ,sasaml6.1,sas visual designer (data prepration,scenario rule creation,tunning,de[loyment) Perl script- 3.3 year. Regular expression, System intrection,wrapper,process manipulation, File system analysis and traversal, Mail processing and filtering, Security considerations, Logfile processing monitering, Interacting with network services Socket programming in perl using DBI to connect different database. perl web development framework: Catalyst,MVC Database : MYsql,postgresql,sqlite Ms-access & vba of excel reporting –advance -2.3 year . Role: Senior Software Engineer & analyst Jan2010 to Jul 2013 Project: Creadit system Role: SAS Analyst Environment: sun solaris10 unix , SAS 9.1.3 Client: city bank Domain: finance Designation – :Software Engineer and analyst.,tableu SAS Procedures Used : Univariate, Means, Freq, Tabulate, Sort, Transpose, Report, Print, Append, Format and Report, shell script , perlscript,vba excel,vba access ORACLE:- 10g,qlikview,performance tuning sql,Rman(backup and recovery) ,pl/sql Description This regarding the customer of the city bank which the analysis the accountancy Of annual/monthly/weekly transaction and what credit to gain and what credit to provide through bank and how many customer should be in this particular line . Responsibilities In online analytical processing (OLAP)- , that is analysis techniques with functionalities such as summarization, consolidation and aggregation as well as the ability to view information from different angles. Although OLAP tool support multidimensional analysis and decision making, additional data analysis through sas- depth analysis such as classification ,clustering and characterization In OLAP I have use. User and system oriented. Data content. Data base design. View Access pattern. Multi Dimensional Data cube. Multi Dimensional Table is organized center them like different table attribute. And this them is repesenteted by fact table Multidimensional table. Virtual warehouse. As we are getting files from across system, we need to format the files. Bringing up the Data Mapping exercise, which gave a solid base to prepare the Technical Specification Document. In getting all issue clarifications by identifying them and conveying it to Onsite through frequent calls and mails and tracking the open issues to closure. Retrieved tables from databases with PROC SQL using ‘Pass through Facility’. Ad-hoc reports are generated from the analysis dataset as per the requirement of the Statisticians using Proc report, Proc tabulate, Proc print and Data Null. Created analysis datasets from raw datasets using Set, Merge, Sort, Update, Formats, Functions and conditional statements Used Data _Null_ technique for producing highly formatted and highly customized reports. Used SAS Macro facility to produce weekly and monthly reports. Involved in identifying the inconsistency of the data using SAS procedures. Involved in writing code to extract clean and validated Data from tables. Listing & summary reports were generated based on the client requirement. Used SAS ODS for generating reports in specific output formats like RTF, PDF, HTML etc Extraction Transformation & Loading Project Project: Etl tool using Type: Performance monitoring software tool. Database: Oracle-11g. Platform: Sun-Solaris.,shell script,perlscript Tools : Toad. Client: Airtel india Responsibilities Deploying performance management solutions encompassing the interface to vendor telecommunications technologies, data transformation, loading and presentation of network performance KPI’s via the etl tool End-user performance engineering software application. Part of Support Team. Performing mediations Monitoring performance and Health Checks of Interface. Finding root cause for mediations error, backlog file and re-processing them by correct them. Checking the status of the system and finding RCA, such that all the jobs are finished . Writing solutions specifications, acceptance test specifications for bespoke customer solutions. Interfacing with the development team for quick resolution of defects. Monitoring Mediation system performance. Developed interfaces and reports using sas and vba excel and vba access. End-user technology report creation Coordinating with Support team,other team . Project: AMWEST Client: US Airways Domain: Airlines Role: SAS Analyst,sas developer,tableu Environment : MAINFRAME, SAS 9.1.3 SAS Procedures : Univariate, Means, Freq, Tabulate, Sort,Transpose, Report, Print, Append, Format and Report. JCL,qlikview Description PA has been working with US Airways team (Previously America West) for over three years in helping support and development of some e-business applications. One of the key applications is the FPS (Flight Profitability System). Our offshore team has build up good knowledge and equity on FPS over the years. This has helped us to gain considerable domain knowledge on the system. Origination of this project is due to the merger of America west airlines with US airways airlines. The existing FPS processes America west data to generate the user reports. The modified FPS system will process both the east (US airways) and west data. A set of requirements has to be completed which are part of a merger between America west airways and US airways. Responsibilities Analysis of the existing applications and documenting the same. Involved in Analysis of functional specifications received from the client. Involved in Preparing/Reviewing the Software Requirement Specifications. Responsible for Coding & Testing. Had given training for Trainees. The peer team members tested each module depending on the Application Retrieved tables from databases with PROC SQL using ‘Pass through Facility’. Ad-hoc reports are generated from the analysis dataset as per the requirement of the Statisticians using Proc report, Proc tabulate, Proc print and Data Null. Created analysis datasets from raw datasets using Set, Merge, Sort, Update, Formats, Functions and conditional statements Used Data _Null_ technique for producing highly formatted and highly customized reports. Used SAS Macro facility to produce weekly and monthly reports. Involved in identifying the inconsistency of the data using SAS procedures. Involved in writing code to extract clean and validated Data from tables. Listing & summary reports were generated based on the client requirement. Used SAS ODS for generating reports in specific output formats like RTF, PDF,HTML etc. Project : TPA records and send reconciliation Role: SAS developer Environment : Windows XP, SAS 9.1.3 SAS Procedures: Univariate, Means, Freq, Tabulate, Sort,Transpose, Report, Print, Append, Format and Report. Description The Hartford is lacking two tools to enable complete and accurate tracking of its TPA business, TPA Sub Ledger and Reconciliation tool. There is no accounting subsystem in place to accurately and completely track TPA financial transactions at a claim level by TPA and by program. This results in dollars being left in corporate general ledger suspense accounts on a monthly basis requiring significant hours of manual reconciliation and research to clear the appropriate account. This is the direct result of lacking the appropriate tools to facilitate research and clearing of the data. Responsibilities Finding out TPA records and send to reconciliation tool As we are getting files from across system, we need to format the files. Bringing up the Data Mapping exercise, which gave a solid base to prepare the Technical Specification Document. In getting all issue clarifications by identifying them and conveying it to Onsite through frequent calls and mails and tracking the open issues to closure. Retrieved tables from databases with PROC SQL using ‘Pass through Facility’. Ad-hoc reports are generated from the analysis dataset as per the requirement of the Statisticians using Proc report, Proc tabulate, Proc print and Data Null. Created analysis datasets from raw datasets using Set, Merge, Sort, Update, Formats, Functions and conditional statements Used Data _Null_ technique for producing highly formatted and highly customized reports. Used SAS Macro facility to produce weekly and monthly reports. Involved in identifying the inconsistency of the data using SAS procedures. Involved in writing code to extract clean and validated Data from tables. Listing & summary reports were generated based on the client requirement. Used SAS ODS for generating reports in specific output formats like RTF, PDF, HTML etc. Project: Phase III Study Management (Hepatitis BVaccine Study) Role: SAS developer Environment: Windows XP, SAS 9.1.3 Client: Revita soft informatics.pvt.ltd SAS Procedures Used : Univariate , Means, Freq, Tabulate, Sort, Transpose,Report, Print, Append, Format and Report. Description The aim of the study is to evaluate the immunogenicity and reactogenicity of an indigenously developed recombinant Hepatitis B vaccine (rHBV) in healthy subjects. This study is a multi centered, phase III trial. Subjects both male and female, who have never been vaccinated with a Hepatitis B vaccine and meeting the admission criteria, are eligible for this study, Blood samples were withdrawn for Hepatitis B markers and Liver function tests were analyzed. The analysis for Demographics included generating Descriptive statistics for the demographic data like age, height, weight, etc. For Safety analysis the local and general side effects were recorded and graded according to severity. Frequencies of reported adverse events categorized were tabulated and listings were created wherever necessary. Responsibilities Retrieved tables from Oracle Clinical database with PROC SQL using ‘Pass through Facility’. Ad-hoc reports are generated from the analysis dataset as per the requirement of the Statisticians and Bio-Statisticians using Proc report, Proc tabulate, Proc print and Data Null. Created analysis datasets from raw datasets using Set, Merge, Sort, Update, Formats, Functions and conditional statements Used Data _Null_ technique for producing highly formatted and highly customized reports. Used SAS Macro facility to produce weekly and monthly reports. Involved in identifying the inconsistency of the data using SAS procedures. Involved in writing code to extract clean and validated Data from tables. Listing & summary reports were generated based on the client requirement. Used SAS ODS for generating reports in specific output formats like RTF, PDF, HTML etc. Nxxit Technology May 2007 to sep 2009 Role: Software Engineer Project : HOME LOAN SYSTEM Role: software engineer Environment: Windows XP Client: North saving bank Language: vs cobol II ,jcl,CICS Database: DB2 Description: Worked- project Project (maintenance / enhancement) Home loan system Staff loan Customer loan Description: this project is maintenance and enhancement for client North saving bank USA. This is one of the module of north saving bank USA. Module of the client:- This project contains various interest rate mica (mortgage interest server account) At the different product levels the rate of interest varies to staff and customers. Role: As a team member I am responsible for Preparing batch program and online program Preparation of test plan Document of new program Unit testing program And coding in cobol , db2 , jcl ,bms or vasm Role: Customization all the Modules like Store, Purchase, Sale, Finance, Production and Costing. Prepared the design specification documents for the above module. Provided support of applications to team members. Developed Reports using Report. Environment:MAINFRAME ( VS COBOL II, db2,jcl,cics) Title: System and network information monitoring for Linux operating environment Tools: HTML, XML Language: PERL Operating system: Linux Customization and Maintenance of different Modules Implemented environment. Prepared the design specification documents for the above module. Provided support of applications to team members. Developed Reports using Report Environment: : linux ,xml, html ,perlscript Prominent Attributes: A quick learner with creativity and enthusiasm. Ability to maintain a good temperament and adaptable to changes. Good organizing ability and communication skills. Ability to work in a team and give at most co-operation. Certification & Qualification Master in computer application from IGNOU. Master in journalism and mass communication from smu. Sun Solaris Certified 10 (UNIX). Diploma in management from IGNOU. Postgraduate diploma in operation management from IGNOU. Bsc. Mathematics in tilkamanjhi university Bhagalpur,Bihar Good knowledge of operation research & statistics Education MCA - IGNOU B.Sc Math(H) - T.M.B. University, Bhagalpur(BIHAR). Contact-Details:- Email:-awinsas@gmail.com awinjes@gmail.com mobile: 9665612536,9013889376 9972548530",awin kumar summary strive excellence field software development dedication focus proactive approach positive attitude passion utilize knowledge skills best possible way fulfillment goals believe like challenging work satisfy instinct learn having high thinking helpful big things motivates anticipate receiving quality work experience provides insight working organization like enrich knowledge fulfil goals technical skills languages sasrpythonpyspark scripting languages shellscript perlscript javascript vbscript xml phpqlikview rdbms oracle db2mysql postgresqlsql server msaccess technologies mainframe aspnet sasstatical analysis system operating system windows nt windows 9x dos linuxunix web application djangoflaskcherrypypyramid system ad tools flow chart dfd database designing tools er diagram normalization dbamongodbperformance tuningclusterrplicationshardingopsmanager project summary analytics project total exp 24 year mainframecobol4yearjcldb2cics unix analytics project 86 year sassasaml61rspssqlik viewtableaumicro strategy pythoncore pythonpandasnumpypysparkrest djangoflask we2py task automation celery rabbitmq perl core perloops analytics shiny year sparkpysparkkafkarabbitmqrddpighivehbasesqoop flumezookeepersparkstreaming unix shell script perl scriptoracleplsqldata warehousing data modelautosys project cisco till role developer tl company tech mahindra project matix cloud 1static code analysis checking venerabilityusing sonarqubepylint 2python web applicationdjangoflaskrestapi database postgresqlmongodb 3sas reporting project 3uk united kingdom aug role dm company quess corp environment awslinux sedawkgrepfindsas 9394enterprise guide python perl data analysistableau qlikviewmicro strategy python django adminrest cicdrabbit mqkafka client stanford graduate school businessusa domain etl designation dm databasedbamongodbperformance tuningclusterrplicationshardingopsmanager sas base sasadvance sas macro qlikviewsas rest spss data representation anlysis hadooppig latinhivehbase mongodbadminpyspark mlib bayesian networkregression model time seriesneural networknlptext miningimage processing cloud awsec2azuregoogle cloud repository gitsvndockerkubernet description data coming different sourclike hdfscsvxmlrdbms file format connected source manipulating data spark sql return data database data display web portel python djangoflaskweb2py time rest api storing retriving data role senior technical consultant dec project stanford graduate school businessusa company skrouz technologies pvtltd environment awslinux sedawkgrepfindsas 9394enterprise guide rpythonperl data analysistablueqlikviewmicrostrategy python django adminrest cicdrabbit mqkafka client stanford graduate school businessusa domain education designation senior technical consultant data scientist database dbamongodbperformance tuningclusterrplicationshardingopsmanager sas base sasadvance sas macro qlikviewsas rest spss data representation anlysis hadooppig latinhivehbase mongodbadminpyspark mlib bayesian networkregression model time series neural networknlptext miningimage processing cloud awsec2azuregoogle cloud repository gitsvndockerkubernet data coming different sourclike hdfscsvxmlrdbms file format connected source manipulating data spark sql return data database data display web portel python djangoflaskweb2py time rest api storing retriving data role consultant oct project absa bank johannesburgsouth africa role consultant company cintana technologies environment linux sedawkgrepfind sas base sasadvance sas macro sasaml61sas visual designerdata preprationscenario rule creationtunningdeloyment rpythonperl data analysistablueqlikviewmicro strategyrabbitmqkafka client absa bankjoburgsouth africa domain banking finance designation consltant data scientist database dbamongodbperformance tuningclusterrplicationshardingopsmanager sas base sasadvance sas macro qlikviewtableumicro startgey spss data representation anlysis hadooppig latinhivehbase mongodbadmin pyspark neural network cloud awsec2azuregoogle cloud repository gitsvndockerkubernet description data coming different sourclike hdfscsvxmlrdbms file format connected source manipulating data spark sql return data database data display web portel python djangoflaskweb2py time rest api storing retriving data role senior software engineer dec sep project citi transaction forcasting engine role senior software engineer company collebra environment linux sedawkgrepfindsas 9394enterprise guide61rpython data analysistablueqlikviewmicro strategy client citi bank domain banking finance designation senior software engineer data scientist sas base sasadvance sas macro qlikviewtableumicro spss data representation anlysis hadooppig latinhivehbase mongodbpyspark mlib bayesian networkregression model time seriesknncaratepplier cloud awsec2azuregoogle cloud repository gitsvndockerkubernet data coming different sourclike hdfscsvxmlrdbms file format connected source manipulating data spark sql return data database data display web portel python djangoflaskweb2py time rest api storing retriving data project e2e aml antimoney laundary role senior software engineer environment linux sedawkgrepfindsas 9394enterprise guide6sas aml data analysistablueqlikviewmicro strategy client citi bank domain banking finance designation senior software engineer data scientist dbamongodbperformance tuningclusterrplicationshardingopsmanager sas base sasadvance sas macro sasaml61sas visual designer data preprationscenario rule creationtunningdeloyment spss data representation anlysis hadooppig latinhivehbase mongodbpyspark mlib bayesian networkregression model time neural netwoknlp text mining cloud awsec2azuregoogle cloud repository gitsvndockerkubernet description data coming different sourclike hdfscsvxmlrdbms file format connected source manipulating data spark sql return data database data display web portel python djangoflaskweb2py time rest api storing retriving data project gtplglobal trade profit loss airsaged inventory reporting system fatsfinancial account trading systemsas compliance role senior software engineer environment linux sedawkgrepfind sas 9394enterprise guide61sasaml61rpython data analysistablueqlikviewmicro strategy client citi bank domain banking finance designation senior software engineer data scientist dbamongodbperformance tuningclusterrplicationshardingopsmanager sas base sasadvance sas macro qlikviewtableurabbitmq spss data representation anlysis hadooppig latinhivehbase mongodbpyspark mlib bayesian networkregression model time seriesknncaratepplier neural networknlp text mining spark scala pysparkrspark analyticsrwithmachine learningpythonipythonnumpypandasmetaplotlib language shellscriptperlpythonrplsql machine leaarning regressionmultiple regressiontimes series analysis shinyr dashboardr data mining description batch job monitoring data analysis dashboard creation cloud awsec2azuregoogle cloud repository gitsvndockerkubernet description data coming different sourclike hdfscsvxmlrdbms file format connected source manipulating data spark sql return data database data display web portel python djangoflaskweb2py time rest api storing retriving data role senior software engineer company adicco india pvtltd project orian2payment gateway role senior software engineer environment aix 71 unixsedawkgrepfind sas 9394enterprise guide61rpython data analysis client bnp paribasibm global business service domain banking finance designation senior software engineer data scientist database dbamongodbperformance tuningclusterrplicationshardingopsmanager sas base sasadvance sas macro spss data representation anlysis haddop pig latinhive hadooppysparkrabbitmq analytics withmachine learningpythonipythonnumpypandasmetaplotlib language shellscriptperlpythonmfcobolrplsql machine leaarning regressionmultiple regressiontimes series analysis shinyr dashboardr data mining unix awksed grep cloud awsec2azuregoogle cloud repository gitsvndockerkubernet description data coming different sourclike hdfscsvxmlrdbms file format connected source manipulating data spark sql return data database data display web portel python djangoflaskweb2py time rest api storing retriving data role senior software engineer nov jul project general motor role senior software engineer company artech info system pvtltd environment sun unixsedawkgrepfindweb scraping sas 9394enterprise guide61rpython data analysisqlikview tableu client general motoribm global business service domain auto mobile designation senior software engineer nov database 92 sas base sasadvance sas macro haddop pig latinhive hadooppyspark analyticsrwithmachine learningpythonipythonnumpypandasmetaplotlib language shellscriptperlpythonmfcobol programingr cloud awsec2azuregoogle cloud repository gitsvndockerkubernet description supply planning system sps sasstock allocation system crrcclient return requesthandles orders process acts intermediary passing essential order information systems aftersales developed sales system solution france italy spssascrr clientserver distributed system runs unix machine located germany spssas crr sites france italy turkey holland hungary run independently spssascrr screens written openroad data stored oracle ingres database batch programs written sql perl cobol sasbase sasadvance sasmacro report generation main functionality supported spssascrr procurement purpose order handling processing acting intermediary passing essential order information systems parts details come dmt dds user initializes sku contracts set spssascrr appropriate supplier mle aware forecasting begins suggested orders generated sent sps approval user sps sends orders supplier sps receives processes asn data suppliers transmitting catalyst system allow processing inbound orders parts received warehouse catalyst transmits confirmed receipts data sps open orders adjusted accordingly stocks reordered according requirements demand role report generationdata analysis production support graphical representation data rsaspythonshiny applications sps supply planning system sas stock allocation system crr client return request report generation country wise client wise country production support kind activity batch job activity implementation shell script perl script programmingmf cobol compression different file system solving ticket production support data analysis python sastableu time series analysis stock allocation system allocate analysis pass data trend analysis seasonal random shiny framework hadoop rrmr2rhdfsr hivarhbase tool rstudior shinygraphical analytic hadooppig latinhive python analysis forecasting database backup recovery mysql database recovery backup postgresql database backup recovery oracle database backup recovery rman mongodb database backup recoverynosql role senior software engineer jul project catarman company artech info system pvtltd environment aixunixsedawkgrepfind sas 9394enterprise guide61rpython data analysis qlikviewtableu client catarmanibmglobal business services domain health care designation senior software engineer database oracledb2mongodbsqoopcasandraqlik view dbamongodbperformance tuningclusterrplicationshardingopsmanager oracle apps functional financialplsql role report generation data analysis mtm report generation rdur report generation data analysis pythonweb scraping numpyipython metaplotlib pandas python language data analysis sas descriptive stastistical analysismeanmedian mode sd inferential stastisticalcorrelationregressionanova testchisquare testing etc toolunix sasvncviewrwinscpshellscriptpythonweb guide 61 sas 9394base sas advance sas macro administration sas management console support added metadata server clusters support added secures libraries new server management functions added support added grid option set support add fro gridlaunched workspace server resource template added revised fro server library definition operating system unixshellscriptperlscript sas sas 9394base sasadvance sasmacro database oracle hadoop project catamaran description converting previous sas code like sas 91 sas 92 sas 9394 creating data set connecting database oracle extraction data preparing report sending multiple address crontab job secheduling getting files system need format files bringing data mapping exercise gave solid base prepare technical specification document getting issue clarifications identifying conveying onsite frequent calls mails tracking open issues closure retrieved tables databases proc sql pass facility adhoc reports generated analysis dataset requirement statisticians proc report proc tabulate proc print data null created analysis datasets raw datasets set merge sort update formats functions conditional statements data null technique producing highly formatted highly customized reports sas macro facility produce weekly monthly reports involved identifying inconsistency data sas procedures involved writing code extract clean validated data tables listing summary reports generated based client requirement sas ods generating reports specific output formats like rtf pdf html etc base sasadvance sas macro sasaml61sas visual designer data preprationscenario rule creationtunningdeloyment perl script 33 year regular expression system intrectionwrapperprocess manipulation file system analysis traversal mail processing filtering security considerations logfile processing monitering interacting network services socket programming perl dbi connect different database perl web development framework catalystmvc database mysqlpostgresqlsqlite msaccess vba excel reporting advance 23 year role senior software engineer analyst jul project creadit system role sas analyst environment sun unix sas 913 client city bank domain finance designation software engineer analysttableu sas procedures univariate means freq tabulate sort transpose report print append format report shell script perlscriptvba excelvba access oracle tuning sqlrmanbackup recovery plsql description customer city bank analysis accountancy annualmonthlyweekly transaction credit gain credit provide bank customer particular line online analytical processing olap analysis techniques summarization consolidation aggregation ability view information different angles olap tool support analysis decision making additional data analysis sas depth analysis classification clustering olap use user system oriented data content data base design view access pattern multi dimensional data cube multi dimensional table organized center like different table attribute repesenteted fact table table virtual warehouse getting files system need format files bringing data mapping exercise gave solid base prepare technical specification document getting issue clarifications identifying conveying onsite frequent calls mails tracking open issues closure retrieved tables databases proc sql pass facility adhoc reports generated analysis dataset requirement statisticians proc report proc tabulate proc print data null created analysis datasets raw datasets set merge sort update formats functions conditional statements data null technique producing highly formatted highly customized reports sas macro facility produce weekly monthly reports involved identifying inconsistency data sas procedures involved writing code extract clean validated data tables listing summary reports generated based client requirement sas ods generating reports specific output formats like rtf pdf html etc extraction transformation loading project project etl tool type performance monitoring software tool database platform sunsolarisshell scriptperlscript tools toad client airtel india deploying performance management solutions encompassing interface vendor technologies data transformation loading presentation network performance kpis etl tool enduser performance engineering software application support team performing mediations monitoring performance health checks interface finding root cause mediations error backlog file reprocessing correct checking status system finding rca jobs finished writing solutions specifications acceptance test specifications bespoke customer solutions interfacing development team quick resolution defects monitoring mediation system performance developed interfaces reports sas vba excel vba access enduser technology report creation coordinating support teamother team project amwest client airways domain airlines role sas analystsas developertableu environment mainframe sas 913 sas procedures univariate means freq tabulate sorttranspose report print append format report jclqlikview description pa working airways team previously america west years helping support development ebusiness applications key applications fps flight profitability system offshore team build good knowledge equity fps years helped gain considerable domain knowledge system origination project merger america west airlines airways airlines existing fps processes america west data generate user reports modified fps system process east airways west data set requirements completed merger america west airways airways analysis existing applications documenting involved analysis functional specifications received client involved preparingreviewing software requirement specifications responsible coding testing given training trainees peer team members tested module depending application retrieved tables databases proc sql pass facility adhoc reports generated analysis dataset requirement statisticians proc report proc tabulate proc print data null created analysis datasets raw datasets set merge sort update formats functions conditional statements data null technique producing highly formatted highly customized reports sas macro facility produce weekly monthly reports involved identifying inconsistency data sas procedures involved writing code extract clean validated data tables listing summary reports generated based client requirement sas ods generating reports specific output formats like rtf pdfhtml etc project tpa records send reconciliation role sas developer environment windows xp sas 913 sas procedures univariate means freq tabulate sorttranspose report print append format report description hartford lacking tools enable complete accurate tracking tpa business tpa sub ledger reconciliation tool accounting subsystem place accurately completely track tpa financial transactions claim level tpa program results dollars left corporate general ledger suspense accounts monthly basis requiring significant hours manual reconciliation research clear appropriate account direct result lacking appropriate tools facilitate research clearing data finding tpa records send reconciliation tool getting files system need format files bringing data mapping exercise gave solid base prepare technical specification document getting issue clarifications identifying conveying onsite frequent calls mails tracking open issues closure retrieved tables databases proc sql pass facility adhoc reports generated analysis dataset requirement statisticians proc report proc tabulate proc print data null created analysis datasets raw datasets set merge sort update formats functions conditional statements data null technique producing highly formatted highly customized reports sas macro facility produce weekly monthly reports involved identifying inconsistency data sas procedures involved writing code extract clean validated data tables listing summary reports generated based client requirement sas ods generating reports specific output formats like rtf pdf html etc project phase iii study management hepatitis bvaccine study role sas developer environment windows xp sas 913 client revita soft informaticspvtltd sas procedures univariate means freq tabulate sort transposereport print append format report description aim study evaluate immunogenicity reactogenicity indigenously developed recombinant hepatitis vaccine rhbv healthy subjects study multi centered phase iii trial subjects male female vaccinated hepatitis vaccine meeting admission criteria eligible study blood samples withdrawn hepatitis markers liver function tests analyzed analysis demographics included generating descriptive statistics demographic data like age height weight etc safety analysis local general effects recorded graded according severity frequencies reported adverse events categorized tabulated listings created necessary retrieved tables oracle clinical database proc sql pass facility adhoc reports generated analysis dataset requirement statisticians biostatisticians proc report proc tabulate proc print data null created analysis datasets raw datasets set merge sort update formats functions conditional statements data null technique producing highly formatted highly customized reports sas macro facility produce weekly monthly reports involved identifying inconsistency data sas procedures involved writing code extract clean validated data tables listing summary reports generated based client requirement sas ods generating reports specific output formats like rtf pdf html etc nxxit technology sep role software engineer project home loan system role software engineer environment windows xp client north saving bank language vs cobol ii jclcics database db2 description worked project project maintenance enhancement home loan system staff loan customer loan description project maintenance enhancement client north saving bank usa module north saving bank usa module client project contains interest rate mica mortgage interest server account different product levels rate interest varies staff customers role team member responsible preparing batch program online program preparation test plan document new program unit testing program coding cobol db2 jcl bms vasm role customization modules like store purchase sale finance production costing prepared design specification documents module provided support applications team members developed reports report environmentmainframe vs cobol ii db2jclcics title system network information monitoring linux operating environment tools html xml language perl operating system linux customization maintenance different modules implemented environment prepared design specification documents module provided support applications team members developed reports report environment linux xml html perlscript prominent attributes quick learner creativity enthusiasm ability maintain good temperament adaptable changes good organizing ability communication skills ability work team cooperation certification qualification master computer application ignou master journalism mass communication smu sun solaris certified unix diploma management ignou postgraduate diploma operation management ignou bsc mathematics tilkamanjhi university bhagalpurbihar good knowledge operation research statistics education mca ignou bsc mathh tmb university bhagalpurbihar contactdetails mobile,['accounting'],0,"['api', 'html', 'flask', 'aws', 'rest', 'javascript', 'database', 'django', 'sql', 'express', 'azure', 'python', 'request']",7,"['react', 'html', 'sass', 'javascript']",0,"['react', 'html', 'sass', 'javascript']",0,['credit'],0,[],0,"['hadoop', 'analytical', 'excel', 'pyspark', 'tableau', 'statistics', 'sql', 'python', 'dbms', 'regression', 'analytics', 'database', 'machine learning', 'data analysis']",7
