filename,name,otherNameHits,email,phoneNo,experience,foundKeywords,numberOfMatch,percentageMatch,original_text,cleaned_text
aniknew16.gmailnaukri.comaniknew16gmailDotcom.pdf,Anik Rastogi,"['Rastogi', 'AWS ETL', 'AWS Redshift', 'Engineer', 'Vidya College']",['[aniknew16@gmail.com]'],['919808165331'],6+ years,"['database', 'analytics', 'hadoop', 'sql', 'data analysis', 'data science', 'r', 'machine learning', 'python', 'regression', 'pyspark', 'dbms', 'analytical']",13,46,"Anik Rastogi sjjsfbujdfjndfndbbfdbfddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd ssjjsfbujdfjndfndbbfdbfddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd sjjsfbujdfjndfndbbfdbfddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd ddddddsdddfeedsxsdddddddfbfjdsfnjsdfnsdfkjndjfndjfndf ddddddsdddfeedsxsdddddddfbfjdsfnjsdfnsdfkjndjfndjfndf +91- 9808165331 @gmail.com aniknew16@gmail.com www.linkedin.com/in/anik-rastogi-86654919 Noida Noida CAREER OBJECTIVE sponsible and challenging position in the organization where my knowledge and experience can be shared and sponsible and challenging position in the organization where my knowledge and experience can be shared and To seek a responsible and challenging position in the organization where my knowledge and experience can be shared and enriched. Enthusiastic Information Technology Analyst with 6+ years experience in complete Product/Service development lifecycle of Enthusiastic Information Technology Analyst with 6+ years experience in complete Product/Service development lifecycle of Enthusiastic Information Technology Analyst with 6+ years experience in complete Product/Service development lifecycle of successfully launched applications. Worked for Top General Insurance Provider Agile and Waterfall Methodology. Insurance from US . Well-versed with Agile and Waterfall Methodology launched applications. Worked for Top General Insurance Provider’s from U.K and Leader of PROFESSIONAL SUMMARY Leader of Healthcare , Apache Airflow, Cron- Jobs, Apache Kafka, Apache Sqoop, Apache Sqoop, Cloudera TECHNICAL SKILLS Programming Languages: Python , PySpark, Cobol Database :DB2, MySql, AWS-Redshift Tools and Technologies : AWS- (S3, EC2,EedR), obol , JCL. Redshift, Hive,hue. Manager, Learning. Basics of Machine Learning PROFESSIONAL EXPERIENCE Tata Consultancy Services (Analyst-Information Humana Inc. (USA) Project Name: Healthcare Insurance Provider - Humana Inc. (USA) Responsibilities: Information-Technology) ( Dec 05, 2018 - till present) ( Dec 05, 2018 Working on Migration of Legacy system to Cloud (AWS).  Working on Migration of Legacy system to Cloud  Created ETL Pipeline to Migrate Data from IBM DB2 Database to AWS ETL Pipeline to Migrate Data from IBM DB2 Database to AWS Redshift Warehouse Redshift Warehouse to prepare data for analytical purpose and for BI layer.  Working as Big Data Developer for Migrating Mainframe Application to Cloud by writing Business Logics from Big Data Developer for Migrating Mainframe Application to Cloud by writing Business Logics from Big Data Developer for Migrating Mainframe Application to Cloud by writing Business Logics from scratch and Driving new insights from data by accessing data from various sources (S3, HDFS, IBM DB2 scratch and Driving new insights from data Database) and processing it through HDFS . ) and processing it through spark with Python(Pyspark) and output outputs again to S3 buckets s sources (S3, HDFS, IBM DB2 and Oracle outputs again to S3 buckets and to  Orchestrating the Pipeline using Airflow. using Airflow.  Coordinating with client to gather requ Coordinating with client to gather requirements and preferences.  Handling training and On-boarding of new developers. of new developers.  inframe system for BAU. Supporting the mainframe system for BAU DXC Technology (Product Developer) AVIVA Plc (UK) Project Name: General Insurance Provider - AVIVA Plc (UK) Responsibilities: eed insurance application(Mainframe).  Developed new features on Exceed insurance application  Worked on Migrating Claims system to Guidewir Worked on Migrating Claims system to Guidewire.  Demonstrated 95% success in timely delivery of stories in sprints. Demonstrated 95% success in timely delivery of stories in sprints.  Performed Unit testing of developed features. Performed Unit testing of developed features.  Hand lined training and On-boarding of new developers. oarding of new developers. Hindustan Computer Limited Technologies (Software Engineer) Hindustan Computer Limited Technologies Project Name: General Insurance Provider – Direct Line Responsibilities: Direct Line Group(U.K)  Analysing Business Requirements, Understanding the Application Design and Component Design Documents Understanding the Application Design and Component Design Documents. Understanding the Application Design and Component Design Documents  Correspondingly developed the system for loped the system for client use on Mainframe System. (Nov 28, 2014 - Jul 04, 2017 ) (Nov 28, 2014 (Jul17, 2017 ul17, 2017 - Nov 12,2018)  Accomplished Unit Testing and supported SIT and UAT.  On-time error analysis reduced downtime by 15% and costs of warranty by up to 25%. ONSITE EXPERIENCE: Experience at client site (London) for Project Implementation and Warranty Support Activities. DATA SCIENCE/ENGINEERING PROJECTS Project Name: Leads Conversion Prediction for online Education Portal Objective: To Predict the Hot Leads for Educational website, so that lead conversion rate can be up-scaled.   Solution: Designed a Supervised machine learning model using Logistic Regression to predict the leads.  Tech Stack: Python  Accomplishment: Developed a model having Sensitivity score of 0.88. Project Name: Countries Clustering International” NGO.  Objective: To categories the countries into clusters based on the Socio-economic and health features for “HELP  Solution: Created the clusters using K-means and Hierarchical Clustering Algorithm’s.  Tech Stack: Python  Accomplishment: Divided the countries into 3 clusters based on Socio-economic and health features. Project Name: ETL Data-Pipelining  Objective: To Perform Business Analysis related to ATM Refilling Issues.  Solution: Created the complete EtL-Pipe line, by fetching the data from RDBMS into Hadoop system, then scanned and processed the data using Pyspark, then written data to AWS-S3 bucket then finally loaded data into amazon redshift to perform business analytics queries  Tech Stack: SQL-Server, Sqoop, Hadoop, Pyspark, AWS-S3, Amazon-Redshift  Accomplishment: Provided the accurate results to all the business queries. Project Name: Retail Data Analysis  Objective: To calculate Key Performance Indicators (KPIs) for an e-commerce company.  Solution: Created the Pipe line, by reading the data from kafka server, then cleaned and processed the data to calculate the performance KPI’s using Pyspark, then writing data on HDFS in Json file format.  Tech Stack: Apache Kafka, Hadoop, Spark Streaming.  Accomplishment: Provided all the required KPI’s data in Json format. Project Name: Online Advertising Platform  Objective: To build an online advertising platform.  Solution: Created the complete end to end data pipeline by creating a module ADS_MANAGER to read ADS data from kafka server processing it and storing it into Sql Server, then created another module ADS_SERVER which will get get request from user device stimulator and will query the sql tables for all the available ads for particular and will display the best AD touser based on Price strategy. Next module was Slot_Budget_Manager whih will update the budgets of every AD according to requests received my Ad_SERVER , Next module was feedback_handler which will receive the user response for the ADS and it will publish it to the Kafka topic, another module Feedback_writer will read the data from kafka topi and will store it into the Hive table , using Sqoop commands have archive the SQL SERVER data into Hive tables, in the last used HUE for some business analysis and reporting.  Tech Stack: Apache Kafka, Hadoop, PySpark , cron jobs, Sql Server, Hive, Hue  Accomplishment: Complete Pipeline was delivered with all the business Reports. EDUCATIONAL QUALIFICATIONS YEAR 2020-2021 2011-2014 SCHOOL/COLLEGE/UNIVERSITY IIIT-Banglore CERTIFICATE PGDDS(Data Engineering) Grads Perusing Vidya College of Engineering, Meerut(U.P) Bachelors of Technology 63.5% ",anik rastogi sjjsfbujdfjndfndbbfdbfddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd ssjjsfbujdfjndfndbbfdbfddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd sjjsfbujdfjndfndbbfdbfddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd ddddddsdddfeedsxsdddddddfbfjdsfnjsdfnsdfkjndjfndjfndf ddddddsdddfeedsxsdddddddfbfjdsfnjsdfnsdfkjndjfndjfndf noida noida career objective sponsible challenging position organization knowledge experience shared sponsible challenging position organization knowledge experience shared seek responsible challenging position organization knowledge experience shared enriched enthusiastic information technology analyst years experience complete productservice development lifecycle enthusiastic information technology analyst years experience complete productservice development lifecycle enthusiastic information technology analyst years experience complete productservice development lifecycle successfully launched applications worked general insurance provider agile waterfall methodology insurance wellversed agile waterfall methodology launched applications worked general insurance providers uk leader professional summary leader healthcare apache airflow cron jobs apache kafka apache sqoop apache sqoop cloudera technical skills programming languages python pyspark cobol database db2 mysql awsredshift tools technologies aws s3 ec2eedr obol jcl redshift hivehue manager learning basics machine learning professional experience tata consultancy services analystinformation humana inc usa project healthcare insurance provider humana inc usa responsibilities informationtechnology dec till present dec working migration legacy system cloud aws working migration legacy system cloud created etl pipeline migrate data ibm db2 database aws etl pipeline migrate data ibm db2 database aws redshift warehouse redshift warehouse prepare data analytical purpose bi layer working big data developer migrating mainframe application cloud writing business logics big data developer migrating mainframe application cloud writing business logics big data developer migrating mainframe application cloud writing business logics scratch driving new insights data accessing data sources s3 hdfs ibm db2 scratch driving new insights data database processing hdfs processing spark pythonpyspark output outputs s3 buckets sources s3 hdfs ibm db2 oracle outputs s3 buckets orchestrating pipeline airflow airflow coordinating client gather requ coordinating client gather requirements preferences handling training onboarding new developers new developers inframe system bau supporting mainframe system bau dxc technology product developer aviva plc uk project general insurance provider aviva plc uk responsibilities eed insurance applicationmainframe developed new features exceed insurance application worked migrating claims system guidewir worked migrating claims system guidewire demonstrated success timely delivery stories sprints demonstrated success timely delivery stories sprints performed unit testing developed features performed unit testing developed features hand lined training onboarding new developers oarding new developers hindustan computer limited technologies software engineer hindustan computer limited technologies project general insurance provider direct line responsibilities direct line groupuk analysing business requirements understanding application design component design documents understanding application design component design documents understanding application design component design documents correspondingly developed system loped system client use mainframe system nov jul nov nov accomplished unit testing supported sit uat ontime error analysis reduced downtime costs warranty onsite experience experience client site london project implementation warranty support activities data scienceengineering projects project leads conversion prediction online education portal objective predict hot leads educational website lead conversion rate upscaled solution designed supervised machine learning model logistic regression predict leads tech stack python accomplishment developed model having sensitivity score project countries clustering international ngo objective categories countries clusters based socioeconomic health features help solution created clusters kmeans hierarchical clustering algorithms tech stack python accomplishment divided countries clusters based socioeconomic health features project etl datapipelining objective perform business analysis related atm refilling issues solution created complete etlpipe line fetching data rdbms hadoop system scanned processed data pyspark written data awss3 bucket finally loaded data amazon redshift perform business analytics queries tech stack sqlserver sqoop hadoop pyspark awss3 amazonredshift accomplishment provided accurate results business queries project retail data analysis objective calculate key performance indicators kpis ecommerce company solution created pipe line reading data kafka server cleaned processed data calculate performance kpis pyspark writing data hdfs json file format tech stack apache kafka hadoop spark streaming accomplishment provided required kpis data json format project online advertising platform objective build online advertising platform solution created complete end end data pipeline creating module adsmanager read ads data kafka server processing storing sql server created module adsserver request user device stimulator query sql tables available ads particular display best ad touser based price strategy module slotbudgetmanager whih update budgets ad according requests received adserver module feedbackhandler receive user response ads publish kafka topic module feedbackwriter read data kafka topi store hive table sqoop commands archive sql server data hive tables hue business analysis reporting tech stack apache kafka hadoop pyspark cron jobs sql server hive hue accomplishment complete pipeline delivered business reports educational qualifications year schoolcollegeuniversity iiitbanglore certificate pgddsdata engineering grads perusing vidya college engineering meerutup bachelors technology
ankitmittal437.gmailnaukri.comAnkitMittal-Resume.pdf,Ankit Mittal,"['Mittal', 'Ankit Mittal', 'Mittal']",['[ankitmittal437@gmail.com]'],['918218825021'],6+ years,"['database', 'analytics', 'data processing', 'sql', 'r', 'python', 'pyspark']",7,25,"Ankit Mittal Email: ankitmittal437@gmail.com Phone: +91-8218825021 Profile Summary ● A dynamic professional with 6+ years of career experience in Data Engineering using Azure Cloud and On Premise Infrastructure, Business Intelligence and Data warehousing ● Currently working with MothersonSumi Infotech and Designs on Azure Data Engineering Profile ● Hands on experience in designing and implementing the architecture for Data Engineering Solutions on Azure Cloud and On Premise ● Hands on experience in data warehouse designing and data modelling ● Expertise on working in Azure Services like Azure Data Factory, Azure SQL Database, Azure Data Lake, Azure Synapse Analytics, Azure Functions, Azure Logic Apps, Azure Databricks, Azure Eventhub, Azure Stream Analytics ● Hands on experience in developing Power BI Reports ● Extensive hands-on experience in Performance Tuning of the Data Warehouse solutions ● Hands-on experience of designing and implementing Real-Time Data Processing Solutions using Microsoft Azure services ● Expertise knowledge of working on Manufacturing Domain along with Hospitality, Market Research, City Gas Distribution domains ● Exposure to working on PySpark (Spark + Python) Professional Experience ● MothersonSumi Infotech and Designs ● Ernst and Young (EY) ● Optimus Information Inc. Professional Projects 1. Factory Analytics Solution (Feb 2020 - Present) (Nov 2019 - Feb 2020) (Jul 2015 - Nov 2019) Technology: Azure Data Factory, Azure SQL Database, T-SQL Queries, Azure Synapse Analytics, Azure Databricks, Azure Data Lake, Azure Eventhub, Azure Stream Analytics, PySpark (Spark + Python) Description: It is a product developed for the Manufacturing units to monitor the plants performance using various KPIs. My role in this project is to design the solution on Microsoft Azure Cloud and also integrate the new clients into the existing on-premise solution. 2. Hospitality Data Analytics Services Technology: Azure Data Factory, Microsoft Power BI, Azure SQL Database, SQL Queries, Powershell, Azure Analytics Services, Azure App Service, Azure Power BI Embedded, Azure Function App Description: Designed, developed and deployed a data engineering solution for the hospitality domain client to keep the track of their Sales, Payments, Taxes, etc. This is a fully cloud based solution implemented on Azure. 3. City Gas Distribution Technology: Azure Data Factory, Azure Databricks, Azure Analysis Service, Power BI, PySpark(Python + Spark) Description: This project is designed to do the analysis on the gas distribution for the City Gas Distribution Company. My role in this project is to develop the Power BI Reports and manage the Analysis Services Model. ● B.Tech (CSE) from Lovely Professional University with 8.3 CGPA in 2015. ● Intermediate from CBSE with 79% in 2011. ● High School from CBSE with 88.2% in 2009. Academic Achievements ● Cleared Microsoft certification 70-761 - Querying Data with Transact-SQL ● Cleared Microsoft certification 70-762 - Developing SQL Databases ● Cleared Microsoft certification 70-767 - Implementing a Data Warehouse ● Cleared Microsoft certification DP-200 - Implementing Azure Data Solutions ● Optimus Tee Ten Award, 2016 ● Optimus Tee Ten Award, 2017 ● Optimus Value Award, Self-Led 2018 Personal Profile ● Date of Birth ● Gender ● Languages : 09-Nov-1993 : Male : English, Hindi Declaration I hereby solemnly affirm that all the information furnished above is true to the best of my knowledge and belief. (Ankit Mittal) Date: 10-Aug-2021 ",ankit mittal email phone profile summary dynamic professional years career experience data engineering azure cloud premise infrastructure business intelligence data warehousing currently working mothersonsumi infotech designs azure data engineering profile hands experience designing implementing architecture data engineering solutions azure cloud premise hands experience data warehouse designing data modelling expertise working azure services like azure data factory azure sql database azure data lake azure synapse analytics azure functions azure logic apps azure databricks azure eventhub azure stream analytics hands experience developing power bi reports extensive handson experience performance tuning data warehouse solutions handson experience designing implementing realtime data processing solutions microsoft azure services expertise knowledge working manufacturing domain hospitality market research city gas distribution domains exposure working pyspark spark python professional experience mothersonsumi infotech designs ernst young ey optimus information inc professional projects factory analytics solution feb present nov feb jul nov technology azure data factory azure sql database tsql queries azure synapse analytics azure databricks azure data lake azure eventhub azure stream analytics pyspark spark python description product developed manufacturing units monitor plants performance kpis role project design solution microsoft azure cloud integrate new clients existing onpremise solution hospitality data analytics services technology azure data factory microsoft power bi azure sql database sql queries powershell azure analytics services azure app service azure power bi embedded azure function app description designed developed deployed data engineering solution hospitality domain client track sales payments taxes etc fully cloud based solution implemented azure city gas distribution technology azure data factory azure databricks azure analysis service power bi pysparkpython spark description project designed analysis gas distribution city gas distribution company role project develop power bi reports manage analysis services model btech cse lovely professional university 83 cgpa intermediate cbse high school cbse academic achievements cleared microsoft certification querying data transactsql cleared microsoft certification developing sql databases cleared microsoft certification implementing data warehouse cleared microsoft certification implementing azure data solutions optimus tee award optimus tee award optimus value award selfled personal profile date birth gender languages male english hindi declaration solemnly affirm information furnished true best knowledge belief ankit mittal date
ansari.adil92.gmailnaukri.comMohdAdilAnsari_Resume.pdf,Adil,"['Ansari Associate', 'Engineer', 'Philips DW', 'Deep']",['[ansari.adil92@gmail.com]'],['918826483728'],6.5 years,"['database', 'sql', 'data science', 'excel', 'r', 'machine learning', 'python', 'deep learning', 'ms excel', 'pyspark', 'dbms']",11,39,"Mohd Adil Ansari Associate, Cognizant, Gurugram, India Email: ansari.adil92@gmail.com Mobile: +918826483728 Overview I have 6.5 Years of software development experience in IT as Data Engineer. As part of my assignments I have built data pipeline using python, kafka, gitlab, PLSQL, PySpark, SparkSQL, and ETL tools in DataWarehousing and Data Integration projects using Agile Scrum methodology. I also have hands-on experience in Machine Learning and Data Science. Experience Summary  Currently working at Cognizant Technology Solutions, since April 2015 within India.  Worked at Metanest Technologies, New Delhi, India for 4 months. Techologies  Programming Languages:- Python, SQL, Oracle PL/SQL, Unix Shell Scrpting  RDBMS:- Oracle, postgreSQL  Devops/Streaming tools : Kafka, gitlab, git, Docker, and Kubernetes  ETL:- Oracle Warehouse Builder, Informatica  ML libraries:- Numpy, Pandas, Scikit-Learn, NLTK  Project Management Tools:- Microsoft office suite, MS Word, MS Excel, PowerPoint, Jira Education Title of the Degree with Branch B.Tech in Information Technology Project and Experience College/University Year of Passing ABES Engineering College 2014 NN Invs-NNIPg– Altis Platform Development of python scripts to Integration of data into relational database, and platform inprovement tasks by Installing, and developing apache airflow jobs. Project Specific Skills Python, Numpy, Pandas, Gitlab, Apache Airflow, PostgreSQL Role and Responsibilities methodologies.  Involved in Requirement Gathering and Analysis from client using agile scrum  Develop python scripts to integrate data into database.  Coding, Installation and configuration of Apache Airflow.  Performation optimization of existing python scripts.  Unit Testing using test cases, and code deployment. Tryg EIM AIA Steady State — DataPipeline Building data pipeline using bigdata technologies in Azure cloud. Structured and Unstructured data is coming from multiple source, transformed, and inserted into Datalake and daa is further used by Analysts, and Data Science, team. Project Specific Skills Python, Gitlab, Kafka, Azure DB, Azure Data Factory, PySpark, SparkSql. Role and Responsibilities methodologies.  Involved in Requirement Gathering and Analysis from client using agile scrum  Developing data pipeline solution as per soecifications.  Unit testing of code by creating unit test cases.  Deployment of application using docker, kubernetes, and Gitlab. Tryg EIM AIA Steady State — DataWarehousing Tryg Fraud project is getting data from DataWarehouse in flat file format using FTP. Data is taken from different subject areas, and alerts are created on data which will be further used by data sceince team for anomaly detection. Project Specific Skills Oracle PL/SQL, Oracle Warehouse Builder (OWB), Shell Scripting, Crontab Role and Responsibilities methodologies.  Involved in Requirement Gathering and Analysis from client using agile scrum  Developing ETL solution using PLSQL, and OWB.  Scheduling ETL Jobs using Crontab, Shell Scripts, and OPC Scheduler.  Guidewire database Integration.  Unit Tesing of code, and Deployment to Production Environment. McAfee DWBI — DataWarehousing McAfee Inc. project sources data from MDM, SAP and Service Portal into the Siebel CRM. The project then builds Data Marts for different Subject Areas and provides a platform for the clients to analyze and improve their Business. Project Specific Skills Informatica PowerCenter8.6, Oracle 11g, DAC. Role and Responsibilities  Involved in Requirement Gathering and Analyses from the client.  Involved in the effort estimation for the project deliverables.  Proposed and designed optimal ETL solution as per the Business Requirement.  Implemented and automated ETL processes using Informatica Power Center tool.  Scheduled and monitored ETL jobs using DAC. Performed Unit and System Testing. Philips Wave2 Steady State — DataWarehousing Philips DW project is getting data from SAP, Flat File, MDM and Oracle Database. After processing of data, data is targeted into SAP and reporting tool for client analysis and further processing of data. Project Specific Skills Informatica PowerCenter9.1, Oracle 11g. Role and Responsibilities solution as per client requirement.  Unit Testing and System Testing.  Ticket creation and resolution in Service Now.  Involved in Requirement Gathering and Analysis from client. Implementation of ETL  Involved in production migration and project migration.  Automated ETL processes using Informatica Power Center tool.  Scheduled and monitored ETL jobs.  Working with different team to make service stable. Professional Certifications Course Name Deep Learning Machine Learning Certification body Year Certificate Link Coursera April 2020 Link Coursera Jan 2019 Link Applied Data Science with Python Coursera Nov 2019 Link ",mohd adil ansari associate cognizant gurugram india email mobile overview 65 years software development experience data engineer assignments built data pipeline python kafka gitlab plsql pyspark sparksql etl tools datawarehousing data integration projects agile scrum methodology handson experience machine learning data science experience summary currently working cognizant technology solutions india worked metanest technologies new delhi india months techologies programming languages python sql oracle plsql unix shell scrpting rdbms oracle postgresql devopsstreaming tools kafka gitlab git docker kubernetes etl oracle warehouse builder informatica ml libraries numpy pandas scikitlearn nltk project management tools microsoft office suite ms word ms excel powerpoint jira education title degree branch btech information technology project experience collegeuniversity year passing abes engineering college nn invsnnipg altis platform development python scripts integration data relational database platform inprovement tasks installing developing apache airflow jobs project specific skills python numpy pandas gitlab apache airflow postgresql role responsibilities methodologies involved requirement gathering analysis client agile scrum develop python scripts integrate data database coding installation configuration apache airflow performation optimization existing python scripts unit testing test cases code deployment tryg eim aia steady state datapipeline building data pipeline bigdata technologies azure cloud structured unstructured data coming multiple source transformed inserted datalake daa analysts data science team project specific skills python gitlab kafka azure db azure data factory pyspark sparksql role responsibilities methodologies involved requirement gathering analysis client agile scrum developing data pipeline solution soecifications unit testing code creating unit test cases deployment application docker kubernetes gitlab tryg eim aia steady state datawarehousing tryg fraud project getting data datawarehouse flat file format ftp data taken different subject areas alerts created data data sceince team anomaly detection project specific skills oracle plsql oracle warehouse builder owb shell scripting crontab role responsibilities methodologies involved requirement gathering analysis client agile scrum developing etl solution plsql owb scheduling etl jobs crontab shell scripts opc scheduler guidewire database integration unit tesing code deployment production environment mcafee dwbi datawarehousing mcafee inc project sources data mdm sap service portal siebel crm project builds data marts different subject areas provides platform clients analyze improve business project specific skills informatica powercenter86 oracle dac role responsibilities involved requirement gathering analyses client involved effort estimation project deliverables proposed designed optimal etl solution business requirement implemented automated etl processes informatica power center tool scheduled monitored etl jobs dac performed unit system testing philips wave2 steady state datawarehousing philips dw project getting data sap flat file mdm oracle database processing data data targeted sap reporting tool client analysis processing data project specific skills informatica powercenter91 oracle role responsibilities solution client requirement unit testing system testing ticket creation resolution service involved requirement gathering analysis client implementation etl involved production migration project migration automated etl processes informatica power center tool scheduled monitored etl jobs working different team service stable professional certifications course deep learning machine learning certification body year certificate link coursera link coursera jan link applied data science python coursera nov link
aparnavarshney96.gmailnaukri.comaparnavarshney96gmailDotcom.pdf,Aparna Varshney,"['Varshney', 'Engineer', 'Engineer', 'Engineer Tata']",['[aparnavarshney96@gmail.co]'],['9711554449'],3+ years,"['data cleaning', 'data processing', 'hadoop', 'sql', 'data analysis', 'r', 'python']",7,25,"Aparna Varshney Data Engineer IT professional with 3+ years of experience conversant with data processing with SQL, Spark, and Hadoop in the Retail sector. Proﬁcient in designing, developing, and testing ETL solutions over large and complex datasets Talend-Big Data Achievements/Tasks aparnavarshney96@gmail.co m 9711554449 Ghaziabad, India linkedin.com/in/aparna- varshney-9a79b0118 TECHNICAL SKILLS Talend-Data integration Abinitio SQL Unix ETL Spark-sql Jira Putty STEP Google Cloud Platform Core-Java Html Python Pandas Numpy Data Cleaning DB Visualizer INTERESTS Reading Books Dancing WORK EXPERIENCE System Engineer Tata Consultancy Services 01/2021 - Present, Achievements/Tasks Engaged as an analyst with a top Retail Company in the US where my role is to analyze the Abinitio(ETL) Graphs, help the team with their doubts with Abinitio and using Stibo Systems Enterprise Platform (STEP), end to end testing of the new application and monitoring logs using Google Big query Application Assistant System Engineer Tata Consultancy Services 04/2019 - 01/2021, Migrated the ETL tool(Abinitio) to Talend as per client’s requirement &developed the ETL pipelines using Talend Spark framework. Worked on Talend migration project to migrate from Abinitio to Talend BigData studio with Hadoop and spark Eﬀectively handled large and complex data sets using big data technologies. Proﬁcient in using SQL for data analysis and creating ETL Solutions Designing, developing and deploying Talend ETL processes on Spark. Experience in UNIX shell scripting, writing UNIX wrapper scripts, and monitoring the UNIX logs to check for any errors Brushed up MVC architecture and testing skills with one of the largest Shipping Organization Ghaziabad Performance Tuning of the Talend Jobs. Assistant System Engineer - Trainee Tata Consultancy Services 08/2018 - 04/2019, Achievements/Tasks EDUCATION B.Tech ABES Engineering College 08/2014 - 07/2018, Courses Computer Science and Engineering(77.5%) CERTIFICATES Google Cloud Digital Leader Certiﬁcation Udemy Hadoop Big Data Extracted high volume data sets from multiple sources, transformed and load to HDFS using Talend BigData studio Worked on various ﬁle system like EBCDIC,ASCII and conversion of EBCDIC ﬁles to ASCII using Talend Understood the Data Flow from DB and Abinitio ETL and transforming them into Logical Talend Jobs using Talend Big Data and Talend DI ",aparna varshney data engineer professional years experience conversant data processing sql spark hadoop retail sector procient designing developing testing etl solutions large complex datasets talendbig data achievementstasks ghaziabad india technical skills talenddata integration abinitio sql unix etl sparksql jira putty step google cloud platform corejava html python pandas numpy data cleaning db visualizer interests reading books dancing work experience system engineer tata consultancy services present achievementstasks engaged analyst retail company role analyze abinitioetl graphs help team doubts abinitio stibo systems enterprise platform step end end testing new application monitoring logs google big query application assistant system engineer tata consultancy services migrated etl toolabinitio talend clients requirement developed etl pipelines talend spark framework worked talend migration project migrate abinitio talend bigdata studio hadoop spark eectively handled large complex data sets big data technologies procient sql data analysis creating etl solutions designing developing deploying talend etl processes spark experience unix shell scripting writing unix wrapper scripts monitoring unix logs check errors brushed mvc architecture testing skills largest shipping organization ghaziabad performance tuning talend jobs assistant system engineer trainee tata consultancy services achievementstasks education btech abes engineering college courses computer science certificates google cloud digital leader certication udemy hadoop big data extracted high volume data sets multiple sources transformed load hdfs talend bigdata studio worked le system like ebcdicascii conversion ebcdic les ascii talend understood data flow db abinitio etl transforming logical talend jobs talend big data talend di
